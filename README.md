ğŸ§  AI Code Assistant (Local LLM with Ollama + Gradio)

A privacy-friendly AI Code Assistant that helps developers understand, debug, refactor
and document code using locally hosted Large Language Models via Ollama. No cloud APIs, 
no data leaving your machine.


âœ¨ Features
	â€¢	ğŸ” Explain code (Python-focused, works for other languages too)
	â€¢	ğŸ› Find bugs & potential issues
	â€¢	ğŸ›  Refactor code for readability & performance
	â€¢	ğŸ“ Auto-generate docstrings and comments
	â€¢	âš¡ Fast local inference using lightweight models (e.g., phi3)
	â€¢	ğŸ” Switch between multiple local LLMs (speed vs quality)
	â€¢	ğŸŒ Clean web UI built with Gradio


ğŸ–¥ï¸ Demo

<img width="1081" height="638" alt="image" src="https://github.com/user-attachments/assets/2c0aa57b-2a2e-4dee-9e01-ecb32ead7456" />



ğŸ§° Tech Stack
	â€¢	Python 3.9+
	â€¢	Ollama (local LLM server)
	â€¢	Gradio (web UI)
	â€¢	Requests (HTTP client)


ğŸš€ Getting Started

1ï¸âƒ£ Install Ollama

Download and install Ollama:
ğŸ‘‰ https://ollama.com

Pull a fast model (recommended):

ollama pull phi3

Other supported models (optional):

ollama pull llama3
ollama pull gemma3:1b

Make sure Ollama is running (it usually runs as a background service on macOS).


2ï¸âƒ£ Clone the Repository

git clone https://github.com/<your-username>/ollama-code-assistant.git
cd ollama-code-assistant



3ï¸âƒ£ Set Up Python Environment

python3 -m venv .venv
source .venv/bin/activate

Install dependencies:

pip install -r requirements.txt


4ï¸âƒ£ Run the App

python app.py

The app will automatically open in your browser at:

http://127.0.0.1:7860


âš™ï¸ Configuration

You can change the default model in app.py:

MODEL_NAME = "phi3"   # fast
# or
MODEL_NAME = "llama3" # higher quality, slower

You can also limit output length for faster responses:

"options": {
    "num_predict": 250
}



ğŸ“ Project Structure

ollama-code-assistant/
â”œâ”€â”€ app.py              # Gradio app + Ollama integration
â”œâ”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ README.md          # Project documentation
â””â”€â”€ .gitignore


ğŸ§  How It Works (High-Level)
	1.	User pastes code into the web UI
	2.	The app builds a task-specific prompt
	3.	Prompt is sent to Ollamaâ€™s local LLM API
	4.	The model generates a response
	5.	The response is displayed in the UI

All processing happens locally on your machine.

ğŸ›£ï¸ Roadmap / Future Improvements
	â€¢	âš¡ Streaming responses (typewriter effect)
	â€¢	ğŸ“„ Upload .py or .js files directly
	â€¢	ğŸ§ª Generate unit tests
	â€¢	ğŸ§  Multi-file context support
	â€¢	ğŸ’¾ Save conversation history
	â€¢	ğŸ§© VS Code plugin integration


ğŸ¤ Contributing

Contributions are welcome!
Feel free to open issues or submit pull requests for improvements and new features.


ğŸ™Œ Acknowledgements
	â€¢	Ollama for enabling local LLMs
	â€¢	Gradio for the simple and elegant UI
